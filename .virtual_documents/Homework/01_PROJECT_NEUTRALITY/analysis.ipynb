





# please make sure you install the packages listed in the requirements.txt file in your environment!
# using pip
# pip install -r requirements.txt
#
# using Conda:
# conda create --name <env_name> --file requirements.txt
#
# some basic imports
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import warnings
import seaborn as sns
from scipy.stats import ttest_ind
from scipy.stats import chi2_contingency








## The subfiles are loaded into their seprate dataframes
df_test = pd.read_csv("task_1/mmlu_data/test.csv", sep=',')
df_x = pd.read_csv("task_1/lm_scores/lm_X.csv", sep=',')
df_y = pd.read_csv("task_1/lm_scores/lm_Y.csv", sep=',')
df_z = pd.read_csv("task_1/lm_scores/lm_Z.csv", sep=',')

##The dataframes' size is printed.
print(f"The respective lengths of the test, X, Y and Z data frame are :{df_test.shape[0]}, {df_x.shape[0]}, {df_y.shape[0]} and {df_z.shape[0]}.")





# A

##pd.set_option('display.max_rows', None) added in order to display the full value counts to obtain all answer formats, disabled afterwards to /
##ensure ease of readability of the jupyter notebook. 
pd.reset_option('display.max_rows')

print("Value counts of df_x:")
print(df_x['result'].value_counts())
print("---------------------------")##added for ease of readability 

print("Value counts of df_y:")
print(df_y['result'].value_counts())
print("---------------------------")

print("Value counts of df_z:")
print(df_z['result'].value_counts())
print("---------------------------")








# A
## Deep copies of the initial (or "dirty") dataframes for filtering
df_x_filtered = df_x.copy()
df_y_filtered = df_y.copy()
df_z_filtered = df_z.copy()

## Dataframe filtering by initially converting all results to strings then using a labda function to fiter out those with a length greater than 10
df_x_filtered['result'] = df_x_filtered['result'].astype(str)
df_x_filtered = df_x_filtered[df_x_filtered['result'].apply(lambda x: len(x) < 10)]

df_y_filtered['result'] = df_y_filtered['result'].astype(str)
df_y_filtered = df_y_filtered[df_y_filtered['result'].apply(lambda x : len(x)<10)]

df_z_filtered['result'] = df_z_filtered['result'].astype(str)
df_z_filtered = df_z_filtered[df_z_filtered['result'].apply(lambda x : len(x)<10)]


# B
def clean_answer(dirtyframe, pattern='Answer: '): ## Modification of all answers with a "Answer :" in front
    return dirtyframe.replace(pattern, '')
def clean_answer2(dirtyframe, pattern = ' '): ## Modification of all answers with unwanted spaces
    return dirtyframe.replace(pattern, '')

## df_x filtering using the above defined functions                     
df_x_filtered['result'] = df_x_filtered['result'].apply(clean_answer)
df_x_filtered['result'] = df_x_filtered['result'].apply(clean_answer2)
df_x_filtered = df_x_filtered[df_x_filtered['result'] != 'nan'] ## filters out all rows with nan from the final data frame
df_x_filtered = df_x_filtered[df_x_filtered['result'] != 'NotSure'] ## filters out all rows with NotSure from the final data frame

## df_y filtering using the above defined functions 
df_y_filtered['result'] = df_y_filtered['result'].apply(clean_answer)
df_y_filtered['result'] = df_y_filtered['result'].apply(clean_answer2)
df_y_filtered = df_y_filtered[df_y_filtered['result'] != 'nan']
df_y_filtered = df_y_filtered[df_y_filtered['result'] != 'NotSure']

## df_z filtering using the above defined functions 
df_z_filtered['result'] = df_z_filtered['result'].apply(clean_answer)
df_z_filtered['result'] = df_z_filtered['result'].apply(clean_answer2)
df_z_filtered = df_z_filtered[df_z_filtered['result'] != 'nan']
df_z_filtered = df_z_filtered[df_z_filtered['result'] != 'NotSure']





data_loss_x = df_x.shape[0] - df_x_filtered.shape[0]
print(f"The initial size of the df_x dataframe was {df_x.shape[0]} compared to a filtered size of {df_x_filtered.shape[0]}.This corresponds to a loss of {data_loss_x} rows of data.")

data_loss_y = df_y.shape[0] - df_y_filtered.shape[0]
print(f"The initial size of the df_y dataframe was {df_y.shape[0]} compared to a filtered size of {df_y_filtered.shape[0]}.This corresponds to a loss of {data_loss_y} rows of data.")

data_loss_z = df_z.shape[0] - df_z_filtered.shape[0]
print(f"The initial size of the df_z dataframe was {df_z.shape[0]} compared to a filtered size of {df_z_filtered.shape[0]}.This corresponds to a loss of {data_loss_z} rows of data.")

total_data_loss = data_loss_x + data_loss_y + data_loss_z
print(f"A total data loss of {total_data_loss} rows of data occured during filtering.")








# A
complete_df_x = pd.merge(df_x_filtered, df_test, on='question_id')
complete_df_y = pd.merge(df_y_filtered, df_test, on='question_id')
complete_df_z = pd.merge(df_z_filtered, df_test, on='question_id')


# B
## The new 'correct' column is created as a boolean defined by the equality between the 'result' and 'answer' columns
complete_df_x['correct'] = complete_df_x['result'] == complete_df_x['answer']
complete_df_y['correct'] = complete_df_y['result'] == complete_df_y['answer']
complete_df_z['correct'] = complete_df_z['result'] == complete_df_z['answer']





# A
## copies of the complete filtered dataframes obtained last question are performed for the new maniuplations
df_x_grouped = complete_df_x.copy()
df_y_grouped = complete_df_y.copy()
df_z_grouped = complete_df_z.copy()

## A groupby subject is initially applied and the size of each subject's question pool is computed. 
## It is then directly reconverted into the new dataframe copies defined above using .reset_index() for ease of manipulation
## The columns are renamed from the automatic 0 that was assigned to them
df_x_grouped = df_x_grouped.groupby('subject').size().reset_index()
df_x_grouped.rename(columns={'subject' : 'subject', 0:'count_x'}, inplace=True) 
df_y_grouped = df_y_grouped.groupby('subject').size().reset_index()
df_y_grouped.rename(columns={'subject' : 'subject', 0:'count_y'}, inplace = True)
df_z_grouped = df_z_grouped.groupby('subject').size().reset_index()
df_z_grouped.rename(columns={'subject' : 'subject', 0:'count_z'}, inplace=True)

## A large grouped dataframe is created cobining the three created above
df_grouped = pd.merge(df_x_grouped, df_y_grouped, on='subject')
df_grouped = pd.merge(df_grouped, df_z_grouped, on='subject')

## Differences from the point of view of each dataframe are computed and added to the dataframe. The "POV" model is the first one mentionned in the column name
## Absolute values are computed from the getgo to simplify subsequent filtering 
df_grouped['difference_xy'] = np.abs((df_grouped['count_y']-df_grouped['count_x'])/df_grouped['count_x'])
df_grouped['difference_xz'] = np.abs((df_grouped['count_z']-df_grouped['count_x'])/df_grouped['count_x'])
df_grouped['difference_yx'] = np.abs((df_grouped['count_x']-df_grouped['count_y'])/df_grouped['count_y'])
df_grouped['difference_yz'] = np.abs((df_grouped['count_z']-df_grouped['count_y'])/df_grouped['count_y'])
df_grouped['difference_zx'] = np.abs((df_grouped['count_x']-df_grouped['count_z'])/df_grouped['count_z'])
df_grouped['difference_zy'] = np.abs((df_grouped['count_y']-df_grouped['count_z'])/df_grouped['count_z'])

## Any difference <0.1 in a given subject for a topic from the point of view of one of the three datamodels results in the row being dropped
df_grouped_filtered = df_grouped.drop(df_grouped[(df_grouped['difference_xy'] < 0.1) & (df_grouped['difference_xz'] < 0.1) & 
        (df_grouped['difference_yx'] < 0.1) & (df_grouped['difference_yz'] < 0.1) & 
        (df_grouped['difference_zx'] < 0.1) & (df_grouped['difference_zy'] < 0.1)].index)
print(df_grouped_filtered['subject'])





df_grouped_filtered['weight_x'] = (df_grouped_filtered['count_y'] + df_grouped_filtered['count_z'])/(df_grouped_filtered['count_x'] + df_grouped_filtered['count_y'] + df_grouped_filtered['count_z'])
df_grouped_filtered['weight_y'] = (df_grouped_filtered['count_x'] + df_grouped_filtered['count_z'])/(df_grouped_filtered['count_x'] + df_grouped_filtered['count_y'] + df_grouped_filtered['count_z'])
df_grouped_filtered['weight_z'] = (df_grouped_filtered['count_y'] + df_grouped_filtered['count_x'])/(df_grouped_filtered['count_x'] + df_grouped_filtered['count_y'] + df_grouped_filtered['count_z'])
df_grouped_filtered

###c de la merde, on prend le minimal et on le "upscale" à celui de la moyenne des deux autres en faisant tot/totdesdeuxautres


# C





# PROVIDED CODE
df_mmlu = pd.read_csv('task_2/lm_scores_mmlu.csv')
df_other = pd.read_csv('task_2/lm_scores_other.csv')





# A

## New dataframes are created for each LLM
## On the mmlu dataset
df_mmlu_X = df_mmlu[df_mmlu['model_name'] == 'X']
df_mmlu_Y = df_mmlu[df_mmlu['model_name'] == 'Y']
df_mmlu_Z = df_mmlu[df_mmlu['model_name'] == 'Z']
## On the 'other' dataset
df_other_X = df_other[df_other['model_name'] == 'X']
df_other_Y = df_other[df_other['model_name'] == 'Y']
df_other_Z = df_other[df_other['model_name'] == 'Z']

## The mean accuracy is computed using .mean()
## On the mmlu dataset
mean_mmlu_accuracy_X = df_mmlu_X['correct'].mean()
mean_mmlu_accuracy_Y = df_mmlu_Y['correct'].mean()
mean_mmlu_accuracy_Z = df_mmlu_Z['correct'].mean()
## On the 'other' dataset
mean_other_accuracy_X = df_other_X['correct'].mean()
mean_other_accuracy_Y = df_other_Y['correct'].mean()
mean_other_accuracy_Z = df_other_Z['correct'].mean()

## The standard error is computed using error = s/sqrt(size) formula
## On the mmlu dataset
error_mmlu_X = df_mmlu_X['correct'].std()/np.sqrt(len(df_mmlu_X))
error_mmlu_Y = df_mmlu_Y['correct'].std()/np.sqrt(len(df_mmlu_Y))
error_mmlu_Z = df_mmlu_Z['correct'].std()/np.sqrt(len(df_mmlu_Z))
## On the 'other dataset
error_other_X = df_other_X['correct'].std()/np.sqrt(len(df_other_X))
error_other_Y = df_other_Y['correct'].std()/np.sqrt(len(df_other_Y))
error_other_Z = df_other_Z['correct'].std()/np.sqrt(len(df_other_Z))

print(f"The mean accuracy and standard error of the X model on MMLU are respectively {mean_mmlu_accuracy_X} and {error_mmlu_X}.")
print(f"The mean accuracy and standard error of the X model on 'other' are respectively {mean_other_accuracy_X} and {error_other_X}.")

print(f"The mean accuracy and standard error of the Y model on MMLU are respectively {mean_mmlu_accuracy_Y} and {error_mmlu_Y}.")
print(f"The mean accuracy and standard error of the Y model on 'other' are respectively {mean_other_accuracy_Y} and {error_other_Y}.")

print(f"The mean accuracy and standard error of the Z model on MMLU are respectively {mean_mmlu_accuracy_Z} and {error_mmlu_Z}.")
print(f"The mean accuracy and standard error of the Z model on 'other' are respectively {mean_other_accuracy_Z} and {error_other_Z}.")


# B

##In each case, a dataframe is created with the data above for each dataset and plt.bar used for the bar graphs

## mmlu
df_mmlu_A = pd.DataFrame({'Model':['X','Y','Z'],
                          'Accuracy' : [mean_mmlu_accuracy_X, mean_mmlu_accuracy_Y, mean_mmlu_accuracy_Z],
                          'error' : [error_mmlu_X, error_mmlu_Y, error_mmlu_Z]})

mmlu_plot = plt.bar(df_mmlu_A['Model'], df_mmlu_A['Accuracy'], yerr=df_mmlu_A['error'], capsize=5, color=['blue', 'orange', 'green'])
plt.bar_label(mmlu_plot, label_type='center')
plt.xlabel('Model')
plt.ylabel('Mean Accuracy')
plt.title('Model accuracy on the mmlu dataset with standard error bars')


## other

df_other_A = pd.DataFrame({'Model':['X','Y','Z'],
                          'Accuracy' : [mean_other_accuracy_X, mean_other_accuracy_Y, mean_other_accuracy_Z],
                          'error' : [error_other_X, error_other_Y, error_other_Z]})

other_plot = plt.bar(df_other_A['Model'], df_other_A['Accuracy'], yerr=df_other_A['error'], capsize=5, color=['blue', 'orange', 'green'])
plt.bar_label(other_plot, label_type='center')
plt.xlabel('Model')
plt.ylabel(' Mean Accuracy')
plt.title('Model accuracy on the "other" dataset with standard error bars')








# A

## Keeping the first hint in mind, we may simply analyse each dataset using the one of the model centric dataframes created in 2.1.A 

## We first obtain the counts of each answer type for both datasets
print(df_mmlu_X['answer'].value_counts())
print(df_other_X['answer'].value_counts())

## New datasets are created
df_mmlu_distribution = pd.DataFrame({'Answer' : ['A', 'B', 'C', 'D'],
                                     'Number' : [1611, 2943, 3403, 3739],
                                     'Source' : ['MMLU', 'MMLU', 'MMLU', 'MMLU']}) 
df_other_distribution = pd.DataFrame({'Answer' : ['A', 'B', 'C', 'D'],
                                     'Number' : [1078, 1116, 924, 641],
                                     'Source' : ['Other', 'Other', 'Other', 'Other']})
df_tot_distribution = pd.concat([df_mmlu_distribution, df_other_distribution], ignore_index=True)

## Plot is made
dist_plot = sns.barplot(x='Answer', y='Number', hue='Source', data=df_tot_distribution)
plt.xlabel('Correct Answer')
plt.ylabel('Prevalence')
plt.title('Distribution of Correct Answers by Dataset')
dist_plot.bar_label(dist_plot.containers[0], fontsize=10)
dist_plot.bar_label(dist_plot.containers[1], fontsize=10) ## I have no idea why this worked to label both the other and mmlu bars
plt.legend(title='Source')




# B

## A contingency table is first created in order to run the chi^2 independence test using the results of #A
## chi2_contingency imported in the first cell with all the other imports

alpha = 0.05

chi2_test_array = np.array([[1611, 1078],
                            [2943, 1116],
                            [3403, 924],
                            [3739, 641]])

print("The null hypothesis H0 states that answer distribution is independent from the dataset.")
print("The alternative hypothesis H1 states that answer distribution is dataset dependent.")

res = chi2_contingency(chi2_test_array)
print(res)

if res.pvalue<alpha :
    print(f"pvalue = {res.pvalue} < alpha = {alpha} : the null hypothesis is rejected, answer distribution is not dataset independent")
if res.pvalue>alpha :
    print(f"pvalue = {res.pvalue} > alpha = {alpha} : the null hypothesis is valid, answer distribution is dataset independent")





# A

## a mean_accuracy function is defined to simply the computation using the groupby function to create new dataframes on which the mean is immediately computed
def mean_accuracy(df, model, dataset):
    mean_accuracy = df.groupby('answer')['correct'].mean()
    for answer, accuracy in mean_accuracy.items():
        print(f"The mean accuracy for model {model} on the {dataset} dataset for option {answer} is {accuracy}")
        
mean_accuracy(df_mmlu_X, 'X', 'mmlu')
mean_accuracy(df_other_X, 'X', '"other"')


# B

## alpha is computed
CI = 0.95
alpha = 1 - CI
### à corriger 
## Previous dataframes are converted to arrays for use in the scipy ttest_ind function 
array_mmlu_X_A = df_mmlu_X_A['correct'].values
array_other_X_A = df_other_X_A['correct'].values

## The test is performed and the p value extracted for conclusion
res2 = ttest_ind(array_mmlu_X_A, array_other_X_A, equal_var=False)
if res2.pvalue<alpha :
    print(f"pvalue = {res2.pvalue} < alpha = {alpha} : the null hypothesis is rejected, performance is not dataset independent")
if res2.pvalue>alpha :
    print(f"pvalue = {res2.pvalue} > alpha = {alpha} : the null hypothesis is valid, performance is dataset independent")





# C

## The alpha value is the same as defined in 2.3B

## In order to model the 'C or D' performance, the dataframes defined in 2.3A are for C and D are combined for each model in order to get the average mean accuracy.
## They are then converted into arrays as in 2.3B

df_mmlu_X_CD = pd.concat([df_mmlu_X_C, df_mmlu_X_D], axis = 0)
array_mmlu_X_CD = df_mmlu_X_CD['correct'].values
res3 = ttest_ind(array_mmlu_X_A, array_mmlu_X_CD, equal_var=False)
if res3.pvalue<alpha :
    print(f"pvalue = {res3.pvalue} < alpha = {alpha} : the null hypothesis is rejected, the performance is dependent on the correct answer on MMLU")
if res3.pvalue>alpha :
    print(f"pvalue = {res3.pvalue} > alpha = {alpha} : the null hypothesis is valid, the performance is not dependent on the correct answer on MMLU")


df_other_X_CD = pd.concat([df_other_X_C, df_other_X_D], axis = 0)
array_other_X_CD = df_other_X_CD['correct'].values
res4 = ttest_ind(array_other_X_A, array_other_X_CD, equal_var=False)
if res4.pvalue<alpha :
    print(f"pvalue = {res4.pvalue} < alpha = {alpha} : the null hypothesis is rejected, the performance is dependent on the correct answer on other")
if res4.pvalue>alpha :
    print(f"pvalue = {res4.pvalue} > alpha = {alpha} : the null hypothesis is valid, the performance is not dependent on the correct answer on other")










# A

## The previously defined mean_accuracy function is reused

mean_accuracy(df_mmlu_X, 'X', 'mmlu')
mean_accuracy(df_other_X, 'X', '"other"')

mean_accuracy(df_mmlu_Y, 'Y', 'mmlu')
mean_accuracy(df_mmlu_Y, 'Y', '"other"')

mean_accuracy(df_mmlu_Z, 'Z', 'mmlu')
mean_accuracy(df_mmlu_Z, 'Z', '"other"')


















## df_mmlu is taken as defined in task 2's prompt 
df_mmlu_shuffled = pd.read_csv('task_2_5/lm_scores_mmlu_shuffle.csv') 

df_mmlu_shuffled_X = df_mmlu_shuffled[df_mmlu_shuffled['model_name'] == 'X']
df_mmlu_shuffled_Y = df_mmlu_shuffled[df_mmlu_shuffled['model_name'] == 'Y']
df_mmlu_shuffled_Z = df_mmlu_shuffled[df_mmlu_shuffled['model_name'] == 'Z']

array_mmlu_shuffled_X = df_mmlu_shuffled_X['correct'].values
array_mmlu_shuffled_Y = df_mmlu_shuffled_Y['correct'].values
array_mmlu_shuffled_Z = df_mmlu_shuffled_Z['correct'].values

array_mmlu_X = df_mmlu_X['correct'].values
array_mmlu_Y = df_mmlu_Y['correct'].values
array_mmlu_Z = df_mmlu_Z['correct'].values

def test_retest_metric(array1, array2, LLM_name):
    trm = 1/len(array1)*np.sum(array1*array2)
    print(f"The test retest metric for LLM {LLM_name} is {trm}.")

test_retest_metric(array_mmlu_shuffled_X, array_mmlu_X, 'X')
test_retest_metric(array_mmlu_shuffled_Y, array_mmlu_Y, 'Y')
test_retest_metric(array_mmlu_shuffled_Z, array_mmlu_Z, 'Z')





# A

## The distribution of answers given can be computed by cretaing new dataframes from the large ones and getting their lengths

def answer_distribution(df, model, dataset):
    for possibility in ['A', 'B', 'C', 'D']:
        df_new = df[df['result'] == possibility]
        distribution = len(df_new)
        print(f"Model {model} answers {possibility} {distribution} times on the {dataset} dataset.")

answer_distribution(df_mmlu_X, 'X', 'mmlu')
answer_distribution(df_other_X, 'X', '"other"')

answer_distribution(df_mmlu_Y, 'Y', 'mmlu')
answer_distribution(df_other_Y, 'Y', '"other"')

answer_distribution(df_mmlu_Z, 'Z', 'mmlu')
answer_distribution(df_other_Z, 'Z', '"other"')

##do we need a graph here?


## Mean accuracy conditioned on the answer
## This question is the "other way around" of question 2.3. The mean_accuracy function can thus be reused with a simple tweak.
def mean_accuracy(df, model, dataset):
    mean_accuracy = df.groupby('result')['correct'].mean()
    for result, mean_accuracy in mean_accuracy.items():
        print(f"The mean accuracy for model {model} on the {dataset} dataset for answers {result} is {mean_accuracy}")

mean_accuracy(df_mmlu_X, 'X', 'mmlu')
mean_accuracy(df_other_X, 'X', '"other"')

mean_accuracy(df_mmlu_Y, 'Y', 'mmlu')
mean_accuracy(df_other_Y, 'Y', '"other"')

mean_accuracy(df_mmlu_Z, 'Z', 'mmlu')
mean_accuracy(df_other_Z, 'Z', '"other"')









# PROVIDED CODE

try:
    import tiktoken
except Exception as e:
    print('installing tiktoken package')
    
    !pip install tiktoken
    
    import tiktoken

def tokenize_text(s):
    enc = tiktoken.encoding_for_model('gpt-4o')
    tokens = enc.encode(str(s))
    return tokens

example_string = 'hello world'
print(f'humans see: "{example_string}" --> language models see: {tokenize_text(example_string)}')





# A


# B


# C





# A








# A









